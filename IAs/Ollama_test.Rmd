---
title: "Testing_R_and_Python_Ollama"
author: "Victor E. Gil-Biraud"
date: "2024-08-21"
output: html_document
---

```{r}
library(reticulate)
library(tictoc)
use_virtualenv("langChain_test")


```


```{python}
from ollama import chat
#from langchain_community.llms import Ollama

query = "State your purpose."
model="gemma2"

messages = [
  {'role': 'user', 'content': query},
]

```

```{python}
response = chat(model=model, messages=messages)

message = response['message']
print(message['content'])
```

```{python}
messages.append({'role': 'user', 'content': 'Are you a multilingual LLM?'})
response = chat(model=model, messages=messages)
message = response['message']
print(message['content'])
messages.append(message)
```


```{python}
query = ''' Coment the following code:

library(reticulate)
library(tictoc)
use_virtualenv("langChain_test")

from ollama import chat
#from langchain_community.llms import Ollama

query = "State your purpose."
model="gemma2"

messages = [
  {'role': 'user', 'content': query},
]

response = chat(model=model, messages=messages)

message = response['message']
print(message['content'])
messages.append({'role': 'user', 'content': query})
response = chat(model=model, messages=messages)
message = response['message']
print(message['content'])
messages.append(message)

'''
```


```{python}
messages = [
  {'role': 'user', 'content': query},
]
```

```{python}
response = chat(model=model, messages=messages)

message = response['message']
print(message['content'])
```

Respuesta:

```python
# Load necessary libraries
library(reticulate) # For interacting with Python packages from R

# Activate a virtual environment named "langChain_test"
use_virtualenv("langChain_test") 

# Import the chat function from the ollama library
from ollama import chat
# Commented out: uncomment if you want to use langchain_community's Ollama implementation instead

# Define the user query
query = "State your purpose."

# Specify the Ollama model to use (e.g., 'gemma2')
model="gemma2"

# Prepare a list of messages for interaction with the model
messages = [
  {'role': 'user', 'content': query}, # Initial message from the user
]

# Send the query to the Ollama model and retrieve the response
response = chat(model=model, messages=messages) 

# Extract the content of the model's response
message = response['message']
print(message['content'])  # Print the model's first response

# Add the model's response to the message history
messages.append({'role': 'user', 'content': query}) # User asks the same question again

# Send the updated query history to the Ollama model for a second response
response = chat(model=model, messages=messages) 
message = response['message']
print(message['content'])  # Print the model's second response

# Add the second response to the message history (this line seems redundant)
messages.append(message)

```


**Explanation:**

This R code uses the `reticulate` library to interact with a Python environment.  It then utilizes the `ollama` library to communicate with an Ollama language model (specifically 'gemma2' in this case).

The code demonstrates a simple conversational interaction:

1. **Initialization:** Sets up the environment, defines the query, and chooses the Ollama model.
2. **First Interaction:** Sends the initial query to the model and prints its response. 
3. **Second Interaction:**  Repeats the same query and again prints the model's response. This highlights how context is maintained in conversational AI.

**Key Points:**

- The use of `use_virtualenv()` suggests that a specific virtual environment is required for this code to run correctly, ensuring isolated dependencies.
- `from ollama import chat` imports the function for interacting with Ollama models. 
- The `messages` list stores a history of user and model interactions in a structured format.

Let me know if you'd like me to elaborate on any specific part or aspect of this code!


```{python}
messages.append({'role': 'user', 'content': 'How can I save your answer to a .md file?'})
response = chat(model=model, messages=messages)
message = response['message']
print(message['content'])
messages.append(message)
```
Respuesta:
```python
# Load necessary libraries
library(reticulate) # For interacting with Python libraries from R
library(tictoc) # For timing code execution

# Activate the virtual environment "langChain_test"
use_virtualenv("langChain_test") 

# Import the chat function from the ollama library
from ollama import chat
# Comment out if not using the langchain_community implementation
#from langchain_community.llms import Ollama

# Define the query to be sent to the model
query = "State your purpose."

# Set the model name to 'gemma2'
model="gemma2"

# Create a list of messages, starting with the user's query
messages = [
  {'role': 'user', 'content': query},
]

# Send the query to the model and store the response
response = chat(model=model, messages=messages)

# Extract the content of the model's response
message = response['message']
print(message['content']) # Print the model's first response

# Add the user's query as a follow-up message to the conversation history
messages.append({'role': 'user', 'content': query})

# Send the updated query to the model and store the response
response = chat(model=model, messages=messages)

# Extract the content of the model's second response
message = response['message']
print(message['content']) # Print the model's second response

# Add the model's second response as a follow-up message to the conversation history
messages.append(message) 



```

**Explanation:**

This code snippet demonstrates a simple interaction with an open-weights language model named "gemma2" using the ollama library in R. Here's a breakdown:

1. **Library Loading and Environment Setup:**
   - `library(reticulate)`: Loads the reticulate package, which enables R to interact with Python libraries.
   - `library(tictoc)`: Loads the tictoc package for measuring code execution time. 
   - `use_virtualenv("langChain_test")`: Activates a virtual environment named "langChain_test" likely containing necessary Python dependencies.

2. **Model and Query Definition:**
   -  `from ollama import chat`: Imports the `chat` function from the ollama library. This function is used to interact with the chosen language model.
   - `model = "gemma2"`: Sets the target language model to "gemma2".

3. **Conversation Flow:**
   - The code defines a list `messages` containing initial messages for the conversation.
   - It uses `chat(model=model, messages=messages)` to send the user's query to the model and receives a response stored in the `response` variable. 
   - It extracts the model's response (`message`) and prints its content.
   - The process is repeated with an updated `messages` list that includes the previous user query, effectively continuing the conversation.

4. **Multilingual Capabilities:**
   - The final line of the code snippet asks if the model is multilingual. This implies that you might be interested in exploring the language support capabilities of "gemma2". 



Let me know if you have any other questions!
```{r}
write(py$messages[[3]]$content, "respuesta_llm.md")
```

